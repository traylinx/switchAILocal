###############################################################################
# switchAILocal - Configuration Golden Reference
#
# This file serves as a comprehensive example of all available settings.
# Copy this to 'config.yaml' and customize it for your needs.
###############################################################################

# =============================================================================
# 1. SERVER SETTINGS
# =============================================================================

# Server host/interface to bind to.
# Default is empty ("") to bind all interfaces (IPv4 + IPv6).
# Use "127.0.0.1" or "localhost" to restrict access to local machine only.
host: ""

# Server port (default: 18080)
port: 18080

# TLS settings for HTTPS. When enabled, the server listens with the provided certificate and key.
tls:
  enable: false
  cert: ""
  key: ""

# Authentication directory (supports ~ for home directory)
# This is where token files and session data are stored.
auth-dir: "~/.switchailocal"

# API keys for authenticating clients to THIS switchAILocal server.
# Requests must include "Authorization: Bearer <key>"
api-keys:
  - "sk-test-123"

# When true, enable authentication for the WebSocket API (/v1/ws).
ws-auth: true

# =============================================================================
# 2. SECURITY & MANAGEMENT
# =============================================================================

# Management API settings (Dashboard and Control endpoints)
remote-management:
  # Whether to allow remote (non-localhost) management access.
  # When false, only localhost can access management endpoints (key still required).
  allow-remote: false

  # Management key. If a plaintext value is provided here, it will be hashed on startup.
  # All management requests (even from localhost) require this key.
  # Leave empty to disable the Management API entirely (404 for all management routes).
  secret-key: ""

  # Disable the bundled management control panel asset download and HTTP route when true.
  disable-control-panel: false

  # GitHub repository for the management control panel assets.
  panel-github-repository: "https://github.com/traylinx/switchAILocal-Management-Center"

# =============================================================================
# 3. CORE PROXY BEHAVIOR
# =============================================================================

# Enable debug-level logging and detailed error messages.
debug: false

# When true, write application logs to rotating files instead of stdout.
logging-to-file: false

# Maximum total size (MB) of log files. Oldest logs are deleted when limit reached.
# Set to 0 to disable limit.
logs-max-total-size-mb: 0

# When false, disable in-memory usage statistics aggregation.
usage-statistics-enabled: false

# Global Proxy URL. Supports socks5/http/https protocols.
# Example: socks5://user:pass@192.168.1.1:1080/
proxy-url: ""

# Number of times to retry a request on specific HTTP errors (403, 408, 500, 502, 503, 504).
request-retry: 3

# When true, unprefixed model requests only use credentials without a prefix.
# If false, unprefixed requests can match any available credential.
force-model-prefix: false

# Streaming behavior (SSE heartbeats and automatic bootstrap retries)
streaming:
  # Heartbeat interval in seconds. 0 to disable.
  keepalive-seconds: 15
  # Number of retries before the first byte is sent to handle auth rotation.
  bootstrap-retries: 2

# Quota exceeded behavior
quota-exceeded:
  switch-project: true # Move to next project/credential if current quota hit.
  switch-preview-model: true # Fallback to preview models if available.

# =============================================================================
# 4. ROUTING STRATEGY
# =============================================================================

# Routing strategy for selecting credentials when multiple match.
routing:
  # Supported values: "round-robin" (default), "fill-first".
  strategy: "round-robin"

  # Priority list for "auto" model resolution strategy.
  # When set, the system checks these models in order and picks the first active one.
  # Supports "provider:model" syntax.
  # auto-model-priority:
  #   - "ollama:gpt-oss:120b-cloud"
  #   - "switchai-chat"
  #   - "gemini-2.5-flash"

# =============================================================================
# 5. PROVIDER CONFIGURATIONS
# =============================================================================

# -- TRAYLINX SwitchAI Cloud --
# Unified access to 100+ cloud models. Get key at https://switchai.traylinx.com
switchai-api-key:
  - api-key: "sk-lf-..."
    base-url: "https://switchai.traylinx.com/v1"
    models:
      - name: "openai/gpt-oss-120b"
        alias: "switchai-fast"
      - name: "deepseek-reasoner"
        alias: "switchai-reasoner"

# -- GOOGLE Gemini API --
gemini-api-key:
  - api-key: "AIzaSy..."
    prefix: "google" # Access via "google/gemini-pro"
    base-url: "https://generativelanguage.googleapis.com"

# -- ANTHROPIC Claude API --
claude-api-key:
  - api-key: "sk-ant-..."
    models:
      - name: "claude-3-5-sonnet-20241022"
        alias: "sonnet"

# -- OPENAI / CODEX API --
codex-api-key:
  - api-key: "sk-..."
    base-url: "https://api.openai.com/v1"

# -- OLLAMA (Local) --
# Start with: ollama serve
ollama:
  enabled: true
  base-url: "http://localhost:11434"
  auto-discover: true # Fetch models from Ollama automatically

# -- OPENCODE (Local) --
opencode:
  enabled: true
  base-url: "http://localhost:4096"
  default-agent: "build"

# -- LM STUDIO (Local) --
lmstudio:
  enabled: false
  base-url: "http://localhost:1234/v1"
  auto-discover: true

# -- OPENAI COMPATIBILITY (Groq, OpenRouter, Together AI, etc.) --
openai-compatibility:
  - name: "groq"
    prefix: "groq"
    base-url: "https://api.groq.com/openai/v1"
    api-key-entries:
      - api-key: "gsk_..."
  - name: "openrouter"
    prefix: "or"
    base-url: "https://openrouter.ai/api/v1"
    api-key-entries:
      - api-key: "sk-or-v1-..."

# -- VERTEX COMPATIBILITY --
# For third-party providers using Vertex-style protocols
# vertex-api-key:
#   - api-key: "vk-..."
#     base-url: "https://api.example.com"

# =============================================================================
# 6. INTELLIGENT ROUTING (CORTEX PHASE 2)
# =============================================================================

# Intelligence is the core of Cortex Router. It manages classification,
# semantic matching, and dynamic model allocation.
intelligence:
  # Master switch for all intelligence features.
  enabled: true

  # Primary model used for request classification.
  router-model: "ollama:gpt-oss:20b-cloud"
  # Fallback model if the primary router model fails.
  router-fallback: "openai:gpt-4o-mini"

  # Static intent-to-model mapping matrix.
  matrix:
    coding: "switchai-chat"
    reasoning: "switchai-reasoner"
    fast: "switchai-fast"
    secure: "ollama:llama3.2"
    vision: "ollama:qwen3-vl:235b-instruct-cloud"

  # -- PHASE 2 FEATURES --

  # Automated model discovery across all providers.
  discovery:
    enabled: true
    refresh-interval: 3600 # seconds
    cache-dir: "~/.switchailocal/cache/discovery"

  # Local embedding engine for semantic features.
  # REQUIRES: libonnxruntime and model download via scripts/download-embedding-model.sh
  embedding:
    enabled: true
    model: "all-MiniLM-L6-v2"

  # Semantic matching bypassing LLM classification for speed.
  semantic-tier:
    enabled: true
    confidence-threshold: 0.85

  # Skill-based prompt augmentation.
  skills:
    enabled: true
    directory: "plugins/cortex-router/skills"

  skill-matching:
    enabled: true
    confidence-threshold: 0.80

  # Semantic Cache for high-speed routing of repeated queries.
  semantic-cache:
    enabled: true
    similarity-threshold: 0.95
    max-size: 10000

  # Confidence scoring for classifications.
  confidence:
    enabled: true

  # Cross-verification of classifications.
  verification:
    enabled: true
    confidence-threshold-low: 0.60
    confidence-threshold-high: 0.90

  # Automatic model cascading based on response quality.
  cascade:
    enabled: true
    quality-threshold: 0.70

  # Feedback collection and metrics.
  feedback:
    enabled: true
    retention-days: 90

# =============================================================================
# 7. SUPERBRAIN (AUTONOMOUS SELF-HEALING)
# =============================================================================

# Transforms switchAILocal into an autonomous gateway with monitoring,
# failure diagnosis, and automatic recovery.
superbrain:
  # Master switch
  enabled: false

  # Operational mode:
  #   - "disabled": Off
  #   - "observe": Monitor only
  #   - "diagnose": Propose actions but don't execute
  #   - "human-in-the-loop": Request approval
  #   - "conservative": Auto-execute safe actions
  #   - "autopilot": Full autonomous recovery
  mode: "disabled"

  component_flags:
    overwatch_enabled: true # Real-time monitoring
    doctor_enabled: true # AI failure diagnosis
    injector_enabled: true # Autonomous stdin response
    recovery_enabled: true # Process restart with corrective flags
    fallback_enabled: true # Intelligent failover
    sculptor_enabled: true # Context optimization

  overwatch:
    silence_threshold_ms: 30000
    log_buffer_size: 50
    heartbeat_interval_ms: 1000
    max_restart_attempts: 2

  doctor:
    model: "gemini-flash"
    timeout_ms: 5000

  stdin_injection:
    mode: "conservative"

  context_sculptor:
    enabled: true
    token_estimator: "tiktoken" # tiktoken or simple
    priority_files: ["README.md", "main.go"]

  fallback:
    enabled: true
    providers: ["geminicli", "gemini", "ollama"]
    min_success_rate: 0.5

# =============================================================================
# 8. ADVANCED PAYLOAD INJECTION
# =============================================================================

# Apply default or override parameters to model payloads globally.
# payload:
#   default: # Only set if missing
#     - models: [{name: "gemini-*", protocol: "gemini"}]
#       params: {"generationConfig.thinkingConfig.thinkingBudget": 32768}
#   override: # Always overwrite
#     - models: [{name: "gpt-*", protocol: "openai"}]
#       params: {"user": "switchAILocal-User"}

# =============================================================================
# 9. PROVIDER-SPECIFIC EXCLUSIONS
# =============================================================================

# Global model exclusions per provider.
# oauth-excluded-models:
#   geminicli:
#     - "*-preview"
#   ollama:
#     - "llama2"
